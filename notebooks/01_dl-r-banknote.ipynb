{"metadata":{"colab":{"name":"01_nb_ch02_01.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#  Banknote classification with fcNN\n\nThis notebook is based on nb_ch02_02 from https://tensorchiefs.github.io/dl_book/ \n\nThanks to Michael Prummer and Lars Bosshard who adpated large parts of the code from Python to R as part of a DL coaching with Beate.\n\n__Goal__: In this notebook you will do your first classification. You will see that fully connected networks without a hidden layer can only learn linar decision boundaries, while fully connected networks with hidden layers are able to learn non-linear decision boundaries.\n\n__Usage__: The idea of the notebook is that you try to understand the provided code. Run it, check the output, and play with it by slightly changing the code. There are some questions the answer along the code\n\n__Dataset__: You work with a banknote data set and classification task. We have 5 features that were extracted from wavelet transformed images of banknotes:\n\n1. variance (continuous feature)\n\n2. skewness (continuous feature)\n\n3. curtosis (continuous feature)\n\n4. entropy (continuous feature)\n\n5. class (binary indicating if the banknote is real or fake)\n\nDon't bother too much how these features exactely came from.\nFor this analysis we only use 2 features.\n\n\nx1: skewness of wavelet transformed image\n\nx2: entropy of wavelet transformed image\n\n__The goal is to classify each banknote to either \"real\" (Y=0) or \"fake\" (Y=1), or in a probabilistic model to predict the probability for \"real\" or \"fake\".__\n\n__Content:__\n\n* visualize the data in a simple scatter plot and color the points by the class label\n* use the Keras library to build a fcNN without hidden layers (logistic regression). Use SGD with the * objective to minimize the crossentropy loss.\n* visualize the learned decision boundary in a 2D plot\n* use the Keras library to build a fcNN with a single hidden layer. Use SGD with the objective to \n* minimize the crossentropy loss.\n* visualize the learned decision boundary in a 2D plot\n* compare the performace and the decision boundaries of the two models\n* stack more hidden layers to the model and playaround with the epochs","metadata":{"_uuid":"b839a0bb-ac21-4ee1-93de-9ff9b5f0617a","_cell_guid":"bcd4a05a-1567-475c-a462-83623d21c441","collapsed":false,"id":"NizZ7atiCceE","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Installing TensorFlow and Keras\n\nThe deep learning packages, TensorFlow and Keras are not installed by default, but can be installed as follows:","metadata":{"_uuid":"462da47e-003c-4ed0-a775-2fe77905a9df","_cell_guid":"9db12abd-dc57-464b-a0c9-d1df36f721fd","id":"M1wzgBlEFPrq","trusted":true}},{"cell_type":"code","source":"#Installing TF and Keras takes approx 2 minutes\nif (FALSE){\n#Installing of tensorflow and keras (needed for colab / not needed for kaggle notebooks)\n    install.packages(\"tensorflow\")\n    install.packages(\"keras\")\n    install.packages(\"RCurl\")\n    install.packages(\"kableExtra\")    \n}\n#proc.time() - ptm\n#devtools::install_github(\"rstudio/keras\") works ok but no tfp\nlibrary(keras)\nlibrary(tensorflow)\ntf$version$VERSION #2.3.0","metadata":{"_uuid":"7861b957-5e5c-4d31-9c3e-b0a7da4e27a3","_cell_guid":"17641d63-c4cf-4b7d-8851-7d366f5968e8","collapsed":false,"id":"goTZsQAy_VbC","outputId":"95b6a9e8-9645-4139-b248-0b66dd74ed6a","execution":{"iopub.status.busy":"2022-09-12T13:00:28.445146Z","iopub.execute_input":"2022-09-12T13:00:28.447241Z","iopub.status.idle":"2022-09-12T13:00:37.139491Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports and loading the data\n\nIn the next cell, we load all the required libraries and functions. \nWe also download the data with the 5 featues from the provided url.","metadata":{"_uuid":"e0a3ca71-529a-4142-b45d-5ba4f28b3f10","_cell_guid":"39b179d6-3a3f-422c-a44a-c90d74e3735e","id":"ZdfzJAx_iD4h","trusted":true}},{"cell_type":"code","source":"library(keras)\nlibrary(RCurl)\nlibrary(ggplot2)\n#library(kableExtra)\nmyurl = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00267/\"\nfname = \"data_banknote_authentication.txt\"\nmyurl = paste0(myurl, fname)\nif(url.exists(myurl)) {\n  dataset = read.table(myurl, sep=\",\")\n}\ndim(dataset)","metadata":{"_uuid":"f2ab61fa-cc45-4f30-91e6-e19e674e55fc","_cell_guid":"6f81c2f9-ae90-4f07-a544-f0e2e4c55fa2","collapsed":false,"id":"cVYwcayKiJ85","outputId":"f403ac34-9850-4df5-d594-7ca6d31ae6ad","execution":{"iopub.status.busy":"2022-09-12T13:01:11.066742Z","iopub.execute_input":"2022-09-12T13:01:11.068137Z","iopub.status.idle":"2022-09-12T13:01:11.857763Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The object _dataset_ has 1372 rows and 5 columns.\n\nLet's extract the two featues _x1: skewness of wavelet transformed image_ and _x2: entropy of wavelet transformed image_. We print the shape and see that we for X we have 1372 oberservations with two featues and for Y there are 1372 binary labels.","metadata":{"_uuid":"bfbb7c12-54eb-428a-9413-1a5bc8312e58","_cell_guid":"fbdb8955-7c90-4a10-9b68-4babc8bf65a2","id":"NKRf-y7qimn1","trusted":true}},{"cell_type":"code","source":"X = dataset[, c(2,4)]\nX = as.matrix(X)\nY = dataset[, 5]\n\nstr(X)\nstr(Y)","metadata":{"_uuid":"d5d215bb-2f29-4867-a1a0-76dc5a649935","_cell_guid":"25a8f936-c2fa-446a-8cf8-ffdecaffa38d","collapsed":false,"id":"x316cOLwjMS_","outputId":"c5c4267c-ec56-4d1f-b262-eabf923fc3ca","execution":{"iopub.status.busy":"2022-09-12T13:01:22.164147Z","iopub.execute_input":"2022-09-12T13:01:22.165539Z","iopub.status.idle":"2022-09-12T13:01:22.410090Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization of the data set\nSince the banknotes are described by only 2 features, we can easily visualize the positions of real and fake banknotes in the 2D feature space. You can see that the boundary between the two classes is not separable by a straight line. A curved boundary line will do better. But even then we cannot expect a perfect seperation.","metadata":{"_uuid":"59814ec0-a387-43d8-b10e-2ab953130b00","_cell_guid":"92bc6605-bd96-4c48-876a-55b08d055df0","id":"dIAH71c7jX3C","trusted":true}},{"cell_type":"code","source":"d.plot = data.frame(skewness = dataset[, 2], \n                    entropy = dataset[, 4], \n                    class = c(\"faked\", \"real\")[dataset[, 5]+1])\nggplot(d.plot, aes(x=skewness, y=entropy, color=class)) + \n  geom_point(size=1) + \n  ggtitle(\"Real and faked banknotes in 2D feature space\") + theme_bw()","metadata":{"_uuid":"18dae78c-64b2-469c-87aa-24011f309448","_cell_guid":"a1a14874-8c05-41eb-bb6d-d080a8f90460","collapsed":false,"id":"xSXtLLlYjn77","outputId":"b45eda02-237d-40ef-f70b-c9868eb6d3d3","execution":{"iopub.status.busy":"2022-09-12T13:02:00.812221Z","iopub.execute_input":"2022-09-12T13:02:00.813710Z","iopub.status.idle":"2022-09-12T13:02:01.227239Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## fcNN with only one neuron\n\nLet’s try to use a single neuron with a sigmoid activation function (also known as logistic regression) as model to seperate the banknotes.","metadata":{"_uuid":"56113ce9-65c2-47da-a38a-a8399a90b0db","_cell_guid":"5bbda260-136a-4375-938c-c3321d290e3e","id":"tEDRg3-bj8we","trusted":true}},{"cell_type":"markdown","source":"### Solution with Tensorflow [optional / advanced]\nSkip to Solution with Keras in first reading\n\nLater, we use the high-level API keras for defing neural networks, which is much easier. Here we show the solution with a little linear algebra and TensorFlow. Feal free to skip to the Keras section if this is not your cup of tea.","metadata":{"_uuid":"f8981456-7380-4dfe-b8a9-1f862e027ce4","_cell_guid":"da8b19b3-7b6e-4235-9c1b-09a2c8c4c1e3","id":"VNdZGaK-k6om","trusted":true}},{"cell_type":"markdown","source":"#### Forward pass\n\nWe begin with defining a network. Try to understand the code. \n\n**Questions (try to answer them before looking at the answers below)**:\n\n* What is the dimension of p1?\n* What is determined by `-(Y*tf$math$log(p1) + (1.-Y)*tf$math$log(1.-p1))` ?","metadata":{"_uuid":"4008fadb-07d9-4dff-99b9-0b5a3d825e70","_cell_guid":"d79efddd-919d-4cb1-83cd-7b7cfe367682","id":"ge3V9EI_k-CO","trusted":true}},{"cell_type":"code","source":"library(tensorflow)\n\n# First change data to format suitable for TF.\nX = k_constant(X) #Makes a tensorflow constant matrix (not trainable)\nY = k_constant(Y, shape=c(1372,1)) #Makes a tensorflow constant matrix (not trainable)\nclass(X)\ndim(X)\ndim(Y)\n\n# Weights and biases (trainable)\nW = k_variable(matrix(c(1,2), nrow = 2, ncol = 1))\nb = k_variable(0)\np1 = tf$sigmoid(tf$matmul(X, W) + b) + 1e-5 #Added 1e-5 for numerical stability\n\nloss = -(Y*tf$math$log(p1) + (1.-Y)*tf$math$log(1.-p1)) \nloss = tf$reduce_mean(loss)\nloss #The loss, single number for given X,Y and W,b","metadata":{"_uuid":"8f1bb131-9cd8-4ef0-9550-c21ab00ea9e1","_cell_guid":"4635599c-e8bd-424a-9e65-fb080cc6f634","collapsed":false,"id":"RsvTb-tYlckx","outputId":"b0d5a952-250c-474c-a4ce-d2174a284bc9","execution":{"iopub.status.busy":"2022-09-12T12:03:50.015863Z","iopub.execute_input":"2022-09-12T12:03:50.018827Z","iopub.status.idle":"2022-09-12T12:03:50.983533Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answers to the questions from above (read only after you gave it a try)**\n\n* p1 is the probability that the provided instance is a \"fake\" (Y=1) banknote\n* the term defines the loss corresponding to the negative log-likelihood of our Bernoulli-Logistic regression model","metadata":{"_uuid":"13084aba-9e7a-47bf-a436-8f6b34f7efd3","_cell_guid":"6b00553f-8b77-4717-bca0-4a21d03d896f","id":"1p457GcIuLGO","trusted":true}},{"cell_type":"markdown","source":"#### Gradient Descent with tape mechanism\n\nFor training, we do gradient descent. To keep things simple, we do not use mini-batches here but use all availibe data to calculate the loss. So we need the gradient for the loss w.r.t. W and b.\n\nHere come the power of TensorFlow, which can calculate the gradient via a so called tape mechanism (see section 3.4.2 in our book) for details. There are certain regions in the code, in which you can ask TF to store information to calculate the gradient afterwards. This information is writen on a \"tape\" called t.\n```\nwith(tf$GradientTape() %as% t, {\n  #In this part of the code stores information to calculate the gradient\n}\n```\n\nAfter you run the code, you can read the gradients from the tape. Note, that you can only do this once and the tape this then broken (burn after reading).\n\n```\ngrads = t$gradient(loss, c(W,b))\n```\n\nThe code below, calculate the gradients w.r.t. the trainable variables, which are the weights W and b. The result is a list, which holds in the first elements the gradients of W and in the second elements the gradients of b.","metadata":{"_uuid":"5c819022-d5f8-4514-8d42-7575e4d229fd","_cell_guid":"6d7a26bb-4d37-4cf7-afa4-f972e615b124","id":"JEsIEOeRmhzb","trusted":true}},{"cell_type":"code","source":"with(tf$GradientTape() %as% t, {\n    p1 = tf$sigmoid(tf$matmul(X, W) + b) + 1e-5 #Forward pass, tape stores  gradients\n    d = -(Y*tf$math$log(p1) + (1.-Y)*tf$math$log(1.-p1))\n    loss = tf$reduce_mean(d)\n  })\nt$gradient(loss, c(W,b))","metadata":{"_uuid":"3f2c4b97-74c2-4831-a883-f51360b60c94","_cell_guid":"e0eceee1-2656-48cf-8820-c9c1a1133395","collapsed":false,"id":"AMxKOviaptFU","outputId":"327e3fd9-df49-4cea-e229-7bf49e18d18b","execution":{"iopub.status.busy":"2022-09-12T12:03:52.874890Z","iopub.execute_input":"2022-09-12T12:03:52.876777Z","iopub.status.idle":"2022-09-12T12:03:52.987356Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So to do gradient descent. We just have to repeat the following code several times. \nIn each repetition we do one step of weight updating.\n\n**Question (try to answer them before looking at the answers below)**: \n\n* What is the learning rate in the code below","metadata":{"_uuid":"f2e10ca6-a32f-437c-89fb-379bd76c815c","_cell_guid":"5cc44a2a-cef4-4cd2-aebc-3d097a5c104a","id":"UibnQZmAp6E_","trusted":true}},{"cell_type":"code","source":"for (i in 1:200){\n  with(tf$GradientTape() %as% t, {\n    p1 = tf$sigmoid(tf$matmul(X, W) + b) + 1e-5 #Forward pass, tape stores  gradients\n    d = -(Y*tf$math$log(p1) + (1.-Y)*tf$math$log(1.-p1))\n    loss = tf$reduce_mean(d)\n  })\n  grads = t$gradient(loss, c(W,b)) #Aks the tape for the gradient w.r.t. W and b\n  W_g = grads[[1]]\n  b_g = grads[[2]]\n  #W = k_variable(W - 0.1 * W_g) #SGD\n  #b = k_variable(b - 0.1 * b_g)\n  # do one step of weight updating:\n  W$assign(W - 0.15 * W_g) #Same as W = k_variable(W - 0.1 * W_g) a bit faster\n  b$assign(b - 0.15 * b_g)\n  if (i %% 20 == 0) {print(paste(i,loss))}\n}\nW\nb","metadata":{"_uuid":"b63a65b0-619d-4466-8b61-2e2ac6c87fa7","_cell_guid":"c78b4ee5-6857-4d36-8072-7bcab306df91","collapsed":false,"id":"aFmxOZA-qGk9","outputId":"4b5d0c75-28b7-49d1-df1c-93267a44ad57","execution":{"iopub.status.busy":"2022-09-12T12:03:54.649397Z","iopub.execute_input":"2022-09-12T12:03:54.651959Z","iopub.status.idle":"2022-09-12T12:04:09.214550Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Anser to the question above**: \n\n* The learning rate is here 0.15","metadata":{"_uuid":"4447f9b4-259a-4699-9974-f4c0e4b12444","_cell_guid":"efe6b7bb-1c57-447f-80c8-0a8adf1221e4","id":"hYiVNGZOxx4L","trusted":true}},{"cell_type":"markdown","source":"### Solution with Keras\n\nWe now build the network using the keras API. First, we define the network, via:","metadata":{"_uuid":"ca7e2ec3-52bc-4f84-8d31-857504281aec","_cell_guid":"aba7d68f-4099-4a33-b9a5-9ebad254283a","id":"C1th0WJ6kIKe","trusted":true}},{"cell_type":"markdown","source":"#### Definition of the model\n\nWe use the sequential API from keras to build the model without hidden layers. \n\n**Questions (try to answer them before looking at the answers below)**: \n\n* Why is the number of parameters in this NN 3?\n* How many parameters would you expect if we would have 5 features?\n* How would the code look if we take all 5 features? \n* How will the decision boundary between the two classes look like?","metadata":{"_uuid":"5cb56482-3d16-4a64-91d4-a0c7bc8259da","_cell_guid":"74b3e915-4cf9-4666-ad1c-bcf5367af8ff","id":"k0ylGBdtskK8","trusted":true}},{"cell_type":"markdown","source":"#### Compiling the network","metadata":{"_uuid":"d1aabf2b-2ceb-4949-acea-233d0a44c526","_cell_guid":"aebe26e9-d8c4-4a05-bcfa-67417844baea","id":"M_c3HVmHtXHT","trusted":true}},{"cell_type":"code","source":"# Definition of the network\nmodel = keras_model_sequential()       # starts the definition of the network\nmodel %>%\n  layer_dense(units = 1,               # adds a new layer to the network with a single neuron\n              input_shape = c(2),      # The input is a tensor of size (batch_size, 2), since we don’t specify the Batch Size now\n              activation = \"sigmoid\")  # chooses the activation function ‘sigmoid’\nsummary(model)","metadata":{"_uuid":"71f52acb-a7ea-447d-8c7a-e2f22f522304","_cell_guid":"19ba4612-4860-494f-a69c-7cfba42399f2","collapsed":false,"id":"KC-qDyWjr099","outputId":"1c330209-4bb6-4985-f6ac-4b748829ace0","execution":{"iopub.status.busy":"2022-09-12T12:04:09.216982Z","iopub.execute_input":"2022-09-12T12:04:09.218444Z","iopub.status.idle":"2022-09-12T12:04:09.293909Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sgd = optimizer_sgd(learning_rate=0.15)           # Defining the stochastic gradient descent optimizer\n\n# compile model                        # compile model, which ends the definition of the model \ncompile(model,\n        loss = 'binary_crossentropy',\n        optimizer = sgd,                     # using the stochastic gradient descent optimizer\n        metrics = c('accuracy')\n        )","metadata":{"_uuid":"28b5c5f2-77a6-44f6-9225-c5c717d5c839","_cell_guid":"d41c0e5d-d04b-4da6-9e6e-f2ff82ef4c8c","collapsed":false,"id":"f4db2ig_th-4","execution":{"iopub.status.busy":"2022-09-12T12:04:10.796058Z","iopub.execute_input":"2022-09-12T12:04:10.797604Z","iopub.status.idle":"2022-09-12T12:04:10.827501Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Fit Network","metadata":{"_uuid":"efd23baf-7f6c-4a13-9c93-4925adba0e29","_cell_guid":"ca09d178-ea2c-4989-a1a9-bbd2db096eac","id":"mLOmzUm_tkJN","trusted":true}},{"cell_type":"code","source":"# Training of the network\nhistory = fit(model, X, Y,              # training of the model using the training data stored in X and Y\n          epochs=400,                   # for 400 epochs\n          batch_size=128,               # fix the batch size to 128 examples\n          verbose=0)","metadata":{"_uuid":"4b5141f6-bbdc-4b65-b2b9-5804c2367220","_cell_guid":"4e8965f8-3898-4360-8504-09b967209785","collapsed":false,"id":"uxI5ijD9tsIO","execution":{"iopub.status.busy":"2022-09-12T12:04:11.914518Z","iopub.execute_input":"2022-09-12T12:04:11.916012Z","iopub.status.idle":"2022-09-12T12:04:21.321754Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the so called learning curve, we plot the accuracy and the loss vs the epochs. You can see that after 100 epochs, we predict around 70% of our data correct and have a loss aorund 0.51 (this values can vary from run to run).","metadata":{"_uuid":"a2f85f40-2cd9-42d7-8996-fe3ef8a0157b","_cell_guid":"236ce9ff-808e-48b3-9ef6-7981fc93c91e","id":"JEj__i3ItyQp","trusted":true}},{"cell_type":"code","source":"plot(history) + geom_point(alpha = 0.5) + theme_bw()","metadata":{"_uuid":"e4dfd220-7425-4953-8921-4a02d48e13d4","_cell_guid":"3cf55da8-fe1a-48f0-b60c-c505181693f5","collapsed":false,"id":"eQ_LPVUDt2cc","outputId":"21d4cf2b-374d-491c-cde9-53668531bb07","execution":{"iopub.status.busy":"2022-09-12T12:04:23.498908Z","iopub.execute_input":"2022-09-12T12:04:23.500556Z","iopub.status.idle":"2022-09-12T12:04:24.141808Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can extract the weights of the model. They should be similat to the approach in TensorFlow.","metadata":{"_uuid":"f76124da-1577-4a3a-a41f-a8144158bc7d","_cell_guid":"2828f284-9832-442f-b2e9-940a58416e95","id":"8k-FWgK7uMHC","trusted":true}},{"cell_type":"code","source":"model$get_weights()","metadata":{"_uuid":"a8f60d2d-79a9-4067-8706-64537b6bbf28","_cell_guid":"d776d9fc-30a2-45a1-b678-ba1203c25eed","collapsed":false,"id":"Bv_SvVbeuLKZ","outputId":"a8ba4df8-5493-444c-b198-6bb05a05c8fd","execution":{"iopub.status.busy":"2022-09-12T12:04:26.236241Z","iopub.execute_input":"2022-09-12T12:04:26.237806Z","iopub.status.idle":"2022-09-12T12:04:26.259405Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plotting the learned decision boundary\n\nLet's visualize which decision boundary was learned by the fcNN with only one output neuron (and no hidden layer). As you can see the decision boundary is a straight line. This is not a coincidence but a general property of a single artificial neuron with a sigmoid as activation function and no hidden layer, also known as logistic regression.","metadata":{"_uuid":"c5fd0eaf-d029-4994-896a-60577e763b56","_cell_guid":"aadc137a-4d81-4805-9a03-7ae19f4a29ea","id":"RMXfrZOqugTO","trusted":true}},{"cell_type":"code","source":"# There is a bug in farver_2.0.1 which is currently shiped in the default colab notebooks\n# We need to install the latest version of the package so that plotModel works\ninstall.packages('farver')\nlibrary(farver)","metadata":{"_uuid":"8e83ab26-c17b-432a-b596-af7f59eadfad","_cell_guid":"eb23ccf0-51c0-4679-b807-97f9c07143ec","collapsed":false,"id":"WP3A_TFJO1AB","outputId":"595c21b6-d47c-49fa-a165-fffc0dadcf34","execution":{"iopub.status.busy":"2022-09-12T12:04:27.794740Z","iopub.execute_input":"2022-09-12T12:04:27.796295Z","iopub.status.idle":"2022-09-12T12:05:16.213583Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define a grid for the 2D feature space\n# predict at each grid point the probability for class 1\nplotModel = function(X, Y, model, title){\n  test.grid = expand.grid(seq(-15, 15, length.out = 100),seq(-9, 3, length.out = 100))\n  test.grid = as.matrix(test.grid)  \n  y.pred = predict(model, test.grid)\n  if(length(dim(y.pred))==2 & dim(y.pred)[2]==2){\n    p.fake = y.pred[, 1]\n  } else {\n     p.fake = 1.0 - y.pred \n  }\n  y.plot = data.frame(skewness=test.grid[,1], entropy=test.grid[,2], class.pred=p.fake)\n  ggplot() +\n    geom_raster(data=y.plot, aes(x=skewness, y=entropy, fill=class.pred)) + \n    scale_fill_distiller(name=\"P(fake)\", palette = \"RdBu\", limits=c(0,1)) +\n    geom_point(data=d.plot, aes(x=skewness, y=entropy, color=class), size=1) +\n    ggtitle(title)\n\n}\nplotModel(X, Y, model, \"fcnn separation without hidden layer\")","metadata":{"_uuid":"8e00989d-97ae-4aa4-a9db-a60f7b3c422a","_cell_guid":"4ca67bb2-21a3-4e4e-a57d-42ad44423075","collapsed":false,"id":"NnTuxucjuqKb","outputId":"4ee231ca-f99a-4c7f-e85f-4541f5dc4389","execution":{"iopub.status.busy":"2022-09-12T12:05:16.216066Z","iopub.execute_input":"2022-09-12T12:05:16.217513Z","iopub.status.idle":"2022-09-12T12:05:17.040380Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Anserwers to the questions above**: \n\n* we have 2 input nodes which are connected to a single output nodes and the weights of these connections give two parameters, in addition we have one bias that gives an additional parameter, resulting all over in 2+1=3 parameters.\n\n* in case of 5 features we would have 5+1=6 parameters.\n\n* the code for 5 features is given in the cell below.\n\n* The decision boundary between the two classes is linear, if the NN has no hidden layers.","metadata":{"_uuid":"d2f332db-3c63-4387-a901-a8b85521dd11","_cell_guid":"eb5aea40-30ec-4caf-a1b4-1696db54dfc0","id":"zH0Vdz0fzzOa","trusted":true}},{"cell_type":"code","source":"# Definition of the network\nmodel = keras_model_sequential()       # starts the definition of the network\nmodel %>%\n  layer_dense(units = 1,               # adds a new layer to the network with a single neuron\n              input_shape = c(5),      # The input is a tensor of size (batch_size, 2), since we don’t specify the Batch Size now\n              activation = \"sigmoid\")  # chooses the activation function ‘sigmoid’\nsummary(model)","metadata":{"_uuid":"1edf3f76-35dc-48e9-98e5-4bf365b4130b","_cell_guid":"1778ce54-7828-46ef-a1d1-1eaecc91254d","collapsed":false,"id":"JxUDnjF00tPY","outputId":"8d616891-90a3-4ed4-b1e6-1e965e2f33a6","execution":{"iopub.status.busy":"2022-09-12T12:05:17.043243Z","iopub.execute_input":"2022-09-12T12:05:17.045112Z","iopub.status.idle":"2022-09-12T12:05:17.083859Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## fcNN with one hidden layer\n\nLooking at the visualization of the data tells us that the boundary between the two classes is not very well captured by a line. Therefore a single neuron is not appropriate to model the probability for a fake banknote based on its two features. To get a more flexible model, we introduce an additional layer between input layer and output layer. This is called hidden layer. Here we use a hidden layer with 8 neurons. We also change the ouputnodes form 1 to 2, to get two ouputs for the probability of real and fake banknote. Because we now have 2 outputs, we use the softmax activation function in the output layer. The softmax activation ensures that the output can be interpreted as a probability (see chapter 2 in our book for details)","metadata":{"_uuid":"a85c16c5-4213-448a-b03b-e82a0cb2e997","_cell_guid":"bd690741-4c0b-46ce-9459-96e82117883d","id":"itmuA-r6zwFk","trusted":true}},{"cell_type":"markdown","source":"### Definition of the network","metadata":{"_uuid":"9e3c85f2-a9a1-4368-9b10-73da913cc3d8","_cell_guid":"809f8812-cbcc-49da-a0fc-7327999fef35","id":"B51HvjCH1GzJ","trusted":true}},{"cell_type":"code","source":"# Definition of the network\nmodel = keras_model_sequential()\nmodel = layer_dense(model, units=8, activation='sigmoid', input_shape=c(2))\nmodel = layer_dense(model, units=2, activation='softmax')\n\n# compile model                        # compile model, which ends the definition of the model \ncompile(model,\n        loss = 'categorical_crossentropy',\n        optimizer = sgd,                     # using the stochastic gradient descent optimizer\n        metrics = c('accuracy')\n        )\nsummary(model)","metadata":{"_uuid":"90e9352b-3c22-446a-b194-ad2ead1b364c","_cell_guid":"d0148aa9-30dc-47f4-afa6-ab5bc394ca2b","collapsed":false,"id":"u0el_EHofLgX","outputId":"a05639a3-7d04-4c12-f2b5-c1d724d729e8","execution":{"iopub.status.busy":"2022-09-12T12:05:17.087296Z","iopub.execute_input":"2022-09-12T12:05:17.088959Z","iopub.status.idle":"2022-09-12T12:05:17.149571Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = to_categorical(dataset[, 5])\nprint(head(Y))\nprint(tail(Y))","metadata":{"_uuid":"010941a0-e2d5-403e-b24b-668c85d9f0c8","_cell_guid":"fa7dde41-1de4-4920-a172-f22d8f9c6826","collapsed":false,"id":"6PAJtVSq1NDD","outputId":"f4815968-1e63-4f8d-c23e-13c1eb358b7c","execution":{"iopub.status.busy":"2022-09-12T12:05:17.152026Z","iopub.execute_input":"2022-09-12T12:05:17.153516Z","iopub.status.idle":"2022-09-12T12:05:17.177502Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fit Network\n\nIn the next cell, train the network. In other words, we tune the parameters (weight, biases) that were initialized randomly with stochastic gradient descent to minimize our loss function (the categorical crossentropy). We set the batchsize to 128 per updatestep and train for 400 epochs.","metadata":{"_uuid":"cbdd5845-6e60-487d-98fc-608a52c547d1","_cell_guid":"e06bc088-ae40-4de2-a26f-4cb61dbf3f4e","id":"Dvk7E3nN1liB","trusted":true}},{"cell_type":"code","source":"history = fit(model, X, Y, epochs=400, batch_size=128, verbose=0)","metadata":{"_uuid":"603acb4b-ed90-4507-bdef-2347f674a276","_cell_guid":"062efc5e-0f9d-4f3d-8b0b-534a4791d5fc","collapsed":false,"id":"ixnkGENZ1cwW","execution":{"iopub.status.busy":"2022-09-12T12:05:17.179909Z","iopub.execute_input":"2022-09-12T12:05:17.181334Z","iopub.status.idle":"2022-09-12T12:05:27.765142Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look again at the leraning curve, we plot the accuracy and the loss vs the epochs. You can see that after 100 epochs, we predict around 86% of our data correct and have a loss aorund 0.29 (this values can vary from run to run). This is already alot better than the model without a hidden layer.","metadata":{"_uuid":"d79f552c-caf4-459d-82a3-c140c4cd7551","_cell_guid":"a1f1c25c-385f-4b42-b7b2-908d40a76c17","id":"QvIayIDZ1r_8","trusted":true}},{"cell_type":"code","source":"plot(history) + geom_point() + theme_bw()","metadata":{"_uuid":"4007748d-4e72-4a0c-b131-f7208938284b","_cell_guid":"a497a3a5-cbd0-4dac-90a8-59026ac34f09","collapsed":false,"id":"Ad-FNMNs1opn","outputId":"89d4a3a1-ffe5-4ee2-eccb-eb62a1633c41","execution":{"iopub.status.busy":"2022-09-12T12:05:27.767566Z","iopub.execute_input":"2022-09-12T12:05:27.769060Z","iopub.status.idle":"2022-09-12T12:05:28.185484Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Boundary\n\nLet's visualize which decision boundary was learned by the fcNN with the hidden layer As you can see the decision boundary is a now curved and not straight anymore. The model (with the hidden layer in the middle) separates the the two classes in the training data better and is able to learn non-linear decision boundaries.","metadata":{"_uuid":"be881c81-8ebf-4ffd-8db9-ac43bf34e53e","_cell_guid":"c5c1247f-bc29-4f9a-8bf8-5daf077ec498","id":"h48hB4vT2CJt","trusted":true}},{"cell_type":"code","source":"plotModel(X, Y, model, 'fcnn separation with hidden layer')","metadata":{"_uuid":"ec703b41-cdae-4566-8e0d-e14542cde744","_cell_guid":"a1a093ee-63dd-4c48-b4b4-958ae1dd3a4d","collapsed":false,"id":"T8i3miLa2Qde","outputId":"6b037833-650c-4f18-e6ee-5edcbb3ba158","execution":{"iopub.status.busy":"2022-09-12T12:05:28.187926Z","iopub.execute_input":"2022-09-12T12:05:28.189413Z","iopub.status.idle":"2022-09-12T12:05:28.960542Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_class = predict(model,X) %>% \n    k_argmax() %>% \n    as.matrix() #to create an R object","metadata":{"_uuid":"963b30e9-1aa9-4a2e-ad14-0ae2e103754f","_cell_guid":"7e8c5e05-9f83-40a2-ba09-956a5d58953c","collapsed":false,"id":"FiTAR38g6wny","execution":{"iopub.status.busy":"2022-09-12T12:05:28.962966Z","iopub.execute_input":"2022-09-12T12:05:28.964468Z","iopub.status.idle":"2022-09-12T12:05:29.067826Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"table(d.plot$class, pred_class)","metadata":{"_uuid":"7ad7a998-a206-417c-8426-1c054b6d56c8","_cell_guid":"cd077427-3505-4b89-8c89-8d8598e0f14f","collapsed":false,"id":"4YBE2uSJ2JDR","outputId":"c746890e-90f9-4888-e3b5-78ffd4c20ec1","execution":{"iopub.status.busy":"2022-09-12T12:05:29.071687Z","iopub.execute_input":"2022-09-12T12:05:29.073622Z","iopub.status.idle":"2022-09-12T12:05:29.092925Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Add more hidden layers and play around with the training epochs\n<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />  \nExercise: Add more hidden layers to the model and play around with the training epochs. What do you observe? Look at the learned decision boundary. How does the loss and the accuracy change?","metadata":{"_uuid":"1e00c024-900c-4f46-bbc5-05e720ebbeb6","_cell_guid":"510b925b-cc50-47e0-a18f-f08b02963894","id":"1AQXS-cPRI0Y","trusted":true}},{"cell_type":"markdown","source":"Important note at the end. Here, we did not split our data into training and test or validation data. This is ok here, since we are mainly interested in the visualization of the decission boundary and not in developing a good prediction model. In real problems, however, you should always use a test-set or validation-set (not used in training) to evaluate the performance. You then can check the loss of the training and validation set after each weight update and for very flexible models you will observe, that the train loss goes to zero, but the validation loss first decreases, but then start in increase if your model starts to overfit the train data.","metadata":{"_uuid":"dabf4ab4-4ad1-478f-9220-745bc4ee1bc1","_cell_guid":"147e1759-2ad5-4b89-a8ab-3ee5773b7f96","id":"iQwqEgl_RLnE","trusted":true}}]}